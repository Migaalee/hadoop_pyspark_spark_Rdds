{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Spark- DataFrames - Project 2\n",
    "\n",
    "\n",
    "\n",
    "This notebook exemplifies the execution of a Spark program in Python, using the DataFrames interface.\n",
    "In this example, spark runs in standalone mode and reads data from the local filesystem, while in cluster mode data is read typically from HDFS dsitributed file system.\n",
    "\n",
    "Spark documentation available at:\n",
    "https://spark.apache.org/docs/2.3.1/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the dataset \"Taxi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-08 17:22:10--  https://www.dropbox.com/s/mi1el58o88hd5u8/Taxi_Trips_151MB.csv?dl=0\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.68.1, 2620:100:6024:1::a27d:4401\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.68.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/raw/mi1el58o88hd5u8/Taxi_Trips_151MB.csv [following]\n",
      "--2020-12-08 17:22:10--  https://www.dropbox.com/s/raw/mi1el58o88hd5u8/Taxi_Trips_151MB.csv\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc5b01a0bd495c758bbe0b7d626b.dl.dropboxusercontent.com/cd/0/inline/BEtGQCZlZyJ_hlIilaBqbJ8VYFShKUiaHH9msio0hxRRRrchs7gAQn6Cps33lq-W7imHzzet_OfF4Ee9RfPY8YUPaakB4qxh57BqBkyXTp0kVnZqUTMSNhGX8Vt8aLg59Ec/file# [following]\n",
      "--2020-12-08 17:22:11--  https://uc5b01a0bd495c758bbe0b7d626b.dl.dropboxusercontent.com/cd/0/inline/BEtGQCZlZyJ_hlIilaBqbJ8VYFShKUiaHH9msio0hxRRRrchs7gAQn6Cps33lq-W7imHzzet_OfF4Ee9RfPY8YUPaakB4qxh57BqBkyXTp0kVnZqUTMSNhGX8Vt8aLg59Ec/file\n",
      "Resolving uc5b01a0bd495c758bbe0b7d626b.dl.dropboxusercontent.com (uc5b01a0bd495c758bbe0b7d626b.dl.dropboxusercontent.com)... 162.125.68.15, 2620:100:6024:15::a27d:440f\n",
      "Connecting to uc5b01a0bd495c758bbe0b7d626b.dl.dropboxusercontent.com (uc5b01a0bd495c758bbe0b7d626b.dl.dropboxusercontent.com)|162.125.68.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 158754777 (151M) [text/plain]\n",
      "Saving to: ‘Taxi_small.csv’\n",
      "\n",
      "Taxi_small.csv      100%[===================>] 151.40M  2.73MB/s    in 46s     \n",
      "\n",
      "2020-12-08 17:22:58 (3.30 MB/s) - ‘Taxi_small.csv’ saved [158754777/158754777]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O Taxi_small.csv https://www.dropbox.com/s/mi1el58o88hd5u8/Taxi_Trips_151MB.csv?dl=0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - How many trips were started in each year present in the data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Year|count|\n",
      "+----+-----+\n",
      "|2013|54409|\n",
      "|2014|74753|\n",
      "|2015|64761|\n",
      "|2016|63628|\n",
      "|2017|50006|\n",
      "|2018|41567|\n",
      "|2019|32797|\n",
      "|2020| 6829|\n",
      "+----+-----+\n",
      "\n",
      "CPU times: user 60 ms, sys: 39.3 ms, total: 99.2 ms\n",
      "Wall time: 10.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('dataframes_exercise1').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try :\n",
    "    lines = sc.textFile('Taxi_small.csv')\n",
    "    logRows = lines.filter( lambda line : len(line) > 0 )   \\\n",
    "                        .map( lambda line : line.split(';') ) \\\n",
    "                        .map( lambda arr : Row(Trip_ID = arr[0], Taxi_ID = arr[1], Year = arr[2].split(\" \")[0].split(\"/\")[2], \\\n",
    "                                               Trip_End = arr[3], Trip_Seconds = arr[4], Trip_Miles = arr[5],\\\n",
    "                                               Pickup_ID = arr[6], Dropoff_ID = arr[7], Pickup_area =  arr[8], \\\n",
    "                                               Dropoff_area = arr[9], Fare = arr[10], Tips = arr[11], Tolls = arr[12], \\\n",
    "                                               Extras = arr[13], Trip_Total= arr[14],Payment_type = arr[15] , \\\n",
    "                                               Company = arr[16], Pickup_Centroid_Lat  = arr[17], \\\n",
    "                                               Pickup_Centroid_Lon = arr[18], Pickup_Centroid_Loc = arr[19], \\\n",
    "                                               Dropoff_Centroid_Lat = arr[20], Dropoff_Centroid_Lon = arr[21], \\\n",
    "                                               Dropoff_Centroid_Lo = arr[22] ))\n",
    "    \n",
    "    logRowsDF = spark.createDataFrame( logRows )\n",
    "    \n",
    "    year =  logRowsDF.groupBy('Year').count()\n",
    "    \n",
    "    year = year.orderBy('Year',ascending=True).show()\n",
    "\n",
    "    \n",
    "    sc.stop()\n",
    "except:\n",
    "    print(err)\n",
    "    sc.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - For each of the 24 hours of the day, how many taxi trips there were, what was their average trip miles and trip total cost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise version 2A - pre-processing during Spark-RDD map transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+--------+\n",
      "| hour|trips|avg_miles|avg_cost|\n",
      "+-----+-----+---------+--------+\n",
      "|01 AM| 8537|     3.22|   14.25|\n",
      "|01 PM|15809|     4.31|   16.57|\n",
      "|02 AM| 6637|     3.12|   13.05|\n",
      "|02 PM|15589|     4.58|   17.01|\n",
      "|03 AM| 4783|     3.42|   13.44|\n",
      "|03 PM|16351|     4.54|    18.0|\n",
      "|04 AM| 3325|     5.26|   16.64|\n",
      "|04 PM|17371|     4.18|   17.19|\n",
      "|05 AM| 3110|     7.81|   22.76|\n",
      "|05 PM|19029|     3.84|   15.63|\n",
      "|06 AM| 4572|     6.85|   20.81|\n",
      "|06 PM|20326|     3.68|   15.07|\n",
      "|07 AM| 8427|     4.79|   17.18|\n",
      "|07 PM|19858|     3.77|   15.68|\n",
      "|08 AM|12819|     3.65|   14.41|\n",
      "|08 PM|17032|     4.15|   16.32|\n",
      "|09 AM|14182|     4.15|   15.31|\n",
      "|09 PM|15428|     4.35|   16.75|\n",
      "|10 AM|13728|     4.27|   16.09|\n",
      "|10 PM|14384|     3.99|   15.88|\n",
      "|11 AM|14911|     4.45|   17.04|\n",
      "|11 PM|12487|     4.01|   15.33|\n",
      "|12 AM|10244|     3.74|   14.56|\n",
      "|12 PM|15821|     4.24|   16.58|\n",
      "+-----+-----+---------+--------+\n",
      "\n",
      "CPU times: user 141 ms, sys: 32.8 ms, total: 173 ms\n",
      "Wall time: 16.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('dataframes_exercise2a').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try :\n",
    "    \n",
    "    lines = sc.textFile('Taxi_small.csv')\n",
    "    logRows = lines.filter(lambda line : len(line) > 0 ) \\\n",
    "        .map(lambda line : line.strip()) \\\n",
    "        .map(lambda line : line.split(';')) \\\n",
    "        .filter(lambda arr: arr[2] != '' and arr[5] != '' and arr[5]!='0') \\\n",
    "        .map(lambda arr: arr[:10]+list([float(arr[10].replace(',',''))])+arr[11:] if (arr[10]!='') else (arr[:10]+list([0])+arr[11:]))\\\n",
    "        .map(lambda arr: arr[:11]+list([float(arr[11].replace(',',''))])+arr[12:] if (arr[11]!='') else (arr[:11]+list([0])+arr[12:]))\\\n",
    "        .map(lambda arr: arr[:12]+list([float(arr[12].replace(',',''))])+arr[13:] if (arr[12]!='') else (arr[:12]+list([0])+arr[13:]))\\\n",
    "        .map(lambda arr: arr[:13]+list([float(arr[13].replace(',',''))])+arr[14:] if (arr[13]!='') else (arr[:13]+list([0])+arr[14:]))\\\n",
    "        .map(lambda arr: arr[:14]+list([float(arr[14].replace(',',''))])+arr[15:] if (arr[14]!='') else (arr[:14]+list([0])+arr[15:]))\\\n",
    "        .filter(lambda arr: arr[10]!=0 or arr[11]!=0 or arr[12]!=0 or arr[13]!=0 or arr[14]!=0) \\\n",
    "        .map(lambda arr: arr if (arr[10]+ arr[11]+ arr[12]+arr[13]==arr[14] )\\\n",
    "                             else (arr[:14]+[arr[10]+ arr[11]+ arr[12]+arr[13]]+arr[15:]))\\\n",
    "        .map( lambda arr : Row( hour = arr[2][-11:-9] + \" \" + arr[2][-2:], miles=float(arr[5].replace(',','')),\\\n",
    "                               cost= arr[14]))\n",
    "    \n",
    "    tripRowsDF = spark.createDataFrame( logRows )\n",
    "    \n",
    "    \n",
    "    taxi = tripRowsDF.groupBy(\"hour\").agg(count('hour').alias('trips'),\n",
    "        round(avg(\"miles\"),2).alias('avg_miles'),\n",
    "        round(avg(\"cost\"),2).alias('avg_cost'))\n",
    "    \n",
    "    taxi.orderBy(\"hour\").show(taxi.count())\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "except Exception as err:\n",
    "    print(err)\n",
    "    sc.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise version 2B - pre-processing of data after DataFrame creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+--------+\n",
      "| hour|trips|avg_miles|avg_cost|\n",
      "+-----+-----+---------+--------+\n",
      "|01 AM| 8528|     3.21|   14.26|\n",
      "|01 PM|15752|     4.28|   16.63|\n",
      "|02 AM| 6634|     3.12|   13.05|\n",
      "|02 PM|15519|     4.55|   17.09|\n",
      "|03 AM| 4774|     3.39|   13.47|\n",
      "|03 PM|16268|      4.5|   18.09|\n",
      "|04 AM| 3296|     5.21|   16.79|\n",
      "|04 PM|17307|     4.15|   17.25|\n",
      "|05 AM| 3052|     7.68|   23.19|\n",
      "|05 PM|18987|     3.82|   15.66|\n",
      "|06 AM| 4516|      6.8|   21.07|\n",
      "|06 PM|20291|     3.66|    15.1|\n",
      "|07 AM| 8348|     4.74|   17.34|\n",
      "|07 PM|19822|     3.74|   15.71|\n",
      "|08 AM|12726|     3.61|   14.51|\n",
      "|08 PM|17005|     4.13|   16.35|\n",
      "|09 AM|14122|     4.12|   15.37|\n",
      "|09 PM|15397|     4.33|   16.78|\n",
      "|10 AM|13690|     4.24|   16.14|\n",
      "|10 PM|14367|     3.98|    15.9|\n",
      "|11 AM|14875|     4.43|   17.09|\n",
      "|11 PM|12472|      4.0|   15.34|\n",
      "|12 AM|10231|     3.73|   14.58|\n",
      "|12 PM|15791|     4.23|   16.61|\n",
      "+-----+-----+---------+--------+\n",
      "\n",
      "CPU times: user 174 ms, sys: 165 ms, total: 338 ms\n",
      "Wall time: 23.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('dataframes_exercise2b').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try :\n",
    "    lines = sc.textFile('Taxi_small.csv')\n",
    "    logRows = lines.filter(lambda line : len(line) > 0 ) \\\n",
    "        .map(lambda line : line.strip()) \\\n",
    "        .map(lambda line : line.split(';')) \\\n",
    "        .map( lambda arr : Row( hour = arr[2][-11:-9] + \" \" + arr[2][-2:], \n",
    "                               miles = arr[5], cost= arr[14], Fare = arr[10], Tips = arr[11], Tolls = arr[12], \\\n",
    "                                               Extras = arr[13]))\n",
    "    \n",
    "    taxistrips = spark.createDataFrame( logRows )\n",
    "    \n",
    "    \n",
    "    processed=taxistrips.where((col('miles')!=\"\")& (col('miles')!=\"0\"))\n",
    "    \n",
    "    processed=processed.withColumn(\"Miles\",regexp_replace(col(\"miles\"), \",\", \"\")).drop(processed.miles)\n",
    "   \n",
    "    \n",
    "    processed=processed.withColumn('fare',when(processed.Fare !='',regexp_replace(col(\"Fare\"), \",\", \"\")).otherwise(0))\\\n",
    "    .drop(processed.Fare)\\\n",
    "    .withColumn('tips',when(processed.Tips !='',regexp_replace(col(\"Tips\"), \",\", \"\")).otherwise(0))\\\n",
    "    .drop(processed.Tips)\\\n",
    "    .withColumn('tolls',when(processed.Tolls !='',regexp_replace(col(\"Tolls\"), \",\", \"\")).otherwise(0))\\\n",
    "    .drop(processed.Tolls)\\\n",
    "    .withColumn('extras',when(processed.Extras !='',regexp_replace(col(\"Extras\"), \",\", \"\")).otherwise(0))\\\n",
    "    .drop(processed.Extras)\\\n",
    "    .withColumn('t_cost',when(processed.cost !='',regexp_replace(col(\"cost\"), \",\", \"\")).otherwise(0))\\\n",
    "    .drop(processed.cost)\n",
    "    \n",
    "    processed=processed.where((col('fare')!=0) | (col('tips')!=0) | (col('tolls')!=0) | (col('extras')!=0) | (col('t_cost')!=0))\n",
    "    \n",
    "    \n",
    "    processed=processed.withColumn('total_cost',when((processed.fare+processed.tips+processed.tolls+processed.extras \\\n",
    "                                  !=processed.t_cost)&(processed.fare+processed.tips+processed.tolls+processed.extras \\\n",
    "                                  !=0),processed.fare+processed.tips+processed.tolls+processed.extras)\\\n",
    "                                   .otherwise(processed.t_cost))\\\n",
    "    .drop(processed.t_cost)\n",
    "    \n",
    "   \n",
    "   \n",
    "    \n",
    "    taxi = processed.groupBy(\"hour\").agg(count('hour').alias('trips'),\n",
    "        round(avg(\"Miles\"),2).alias('avg_miles'),\n",
    "        round(avg(\"total_cost\"),2).alias('avg_cost'))\n",
    "    \n",
    "    taxi.orderBy(\"hour\").show(taxi.count())\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "except Exception as err:\n",
    "    print(err)\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 For each of the 24 hours of the day, which are the (up to) 5 most popular routes (pairs pickup/dropoff regions) according to the the total number of taxi trips? Also reportand the average fare (total trip cost)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise version 3A - pre-processing dataset during Spark-RDD map transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----------+------+---------+--------+\n",
      "| hour|    Pick_up|   Drop_off|#trips|avg_miles|avg_cost|\n",
      "+-----+-----------+-----------+------+---------+--------+\n",
      "|01 AM|17031081700|17031081700|    63|      0.7|    7.15|\n",
      "|01 AM|17031081700|17031081800|    56|     0.71|    6.94|\n",
      "|01 AM|17031081700|17031839100|    35|     0.92|     7.9|\n",
      "|01 AM|17031081700|17031320100|    34|     0.94|     7.6|\n",
      "|01 AM|17031081800|17031081800|    32|     0.62|    5.97|\n",
      "|01 PM|17031839100|17031839100|   316|     0.74|    6.83|\n",
      "|01 PM|17031320100|17031839100|   219|     0.91|    7.53|\n",
      "|01 PM|17031839100|17031320100|   215|     0.98|    7.48|\n",
      "|01 PM|17031081500|17031839100|   144|      1.1|    8.01|\n",
      "|01 PM|17031081700|17031839100|   136|     0.76|    7.13|\n",
      "|02 AM|17031081700|17031081800|    53|     0.62|    6.63|\n",
      "|02 AM|17031081700|17031081700|    44|     0.74|    6.91|\n",
      "|02 AM|17031081700|17031320100|    35|     0.95|    7.75|\n",
      "|02 AM|17031081800|17031081800|    27|     1.15|     8.4|\n",
      "|02 AM|17031081800|17031081700|    27|     0.55|    7.56|\n",
      "|02 PM|17031839100|17031839100|   332|     0.87|    7.11|\n",
      "|02 PM|17031839100|17031320100|   184|     0.93|     7.3|\n",
      "|02 PM|17031320100|17031839100|   170|     4.36|    7.82|\n",
      "|02 PM|17031839100|17031980000|   154|    16.16|   48.77|\n",
      "|02 PM|17031081500|17031839100|   152|     1.37|     8.4|\n",
      "|03 AM|17031081700|17031081800|    37|     0.78|    7.23|\n",
      "|03 AM|17031081700|17031081700|    25|     1.18|    8.39|\n",
      "|03 AM|17031081800|17031081700|    21|     0.89|     6.7|\n",
      "|03 AM|17031081700|17031839100|    20|     0.81|    7.06|\n",
      "|03 AM|17031081800|17031081800|    17|     0.85|     7.2|\n",
      "|03 PM|17031839100|17031839100|   310|     0.92|    6.67|\n",
      "|03 PM|17031320100|17031839100|   197|      1.0|    7.65|\n",
      "|03 PM|17031839100|17031320100|   195|     0.87|    6.91|\n",
      "|03 PM|17031839100|17031281900|   185|     0.82|    6.13|\n",
      "|03 PM|17031839100|17031980000|   163|     15.5|   50.61|\n",
      "|04 AM|17031081700|17031081800|    25|     0.55|    6.35|\n",
      "|04 AM|17031839100|17031980000|    19|    16.84|   45.89|\n",
      "|04 AM|17031081500|17031980000|    16|    17.93|    46.2|\n",
      "|04 AM|17031081403|17031980000|    12|     17.7|   45.09|\n",
      "|04 AM|17031081800|17031081800|    11|     0.45|    5.76|\n",
      "|04 PM|17031839100|17031839100|   366|     0.76|    6.51|\n",
      "|04 PM|17031839100|17031281900|   260|     0.75|    6.23|\n",
      "|04 PM|17031839100|17031320100|   237|     0.84|    7.04|\n",
      "|04 PM|17031320100|17031839100|   199|     0.99|    7.72|\n",
      "|04 PM|17031839100|17031081500|   142|     1.25|    8.59|\n",
      "|05 AM|17031320100|17031980000|    59|    17.03|   50.14|\n",
      "|05 AM|17031081500|17031980000|    42|     15.3|   48.51|\n",
      "|05 AM|17031839100|17031980000|    32|     17.9|   48.38|\n",
      "|05 AM|17031081401|17031980000|    19|    16.97|   48.69|\n",
      "|05 AM|17031980000|17031980000|    18|    10.14|   29.35|\n",
      "|05 PM|17031839100|17031839100|   329|     0.82|    6.96|\n",
      "|05 PM|17031839100|17031281900|   264|     0.72|    6.54|\n",
      "|05 PM|17031839100|17031320100|   228|     0.89|    7.28|\n",
      "|05 PM|17031320100|17031839100|   164|     0.98|    8.11|\n",
      "|05 PM|17031839100|17031280100|   157|     0.85|    6.92|\n",
      "|06 AM|17031320100|17031980000|    68|    19.28|   49.26|\n",
      "|06 AM|17031081500|17031980000|    64|    16.64|   46.96|\n",
      "|06 AM|17031320100|17031839100|    56|      1.0|    6.34|\n",
      "|06 AM|17031839100|17031980000|    54|     16.6|   50.01|\n",
      "|06 AM|17031281900|17031839100|    50|     0.77|    5.64|\n",
      "|06 PM|17031839100|17031839100|   252|     0.73|    6.72|\n",
      "|06 PM|17031839100|17031320100|   188|     1.16|    7.67|\n",
      "|06 PM|17031839100|17031081500|   186|     1.18|    8.75|\n",
      "|06 PM|17031839100|17031081700|   181|     0.95|    8.08|\n",
      "|06 PM|17031839100|17031281900|   169|     0.65|    6.35|\n",
      "|07 AM|17031281900|17031839100|   183|     0.76|    5.98|\n",
      "|07 AM|17031320100|17031839100|   175|     0.92|    6.58|\n",
      "|07 AM|17031839100|17031839100|   152|      0.7|    6.17|\n",
      "|07 AM|17031280100|17031839100|   145|     0.73|    6.08|\n",
      "|07 AM|17031081500|17031839100|   122|      1.3|    8.49|\n",
      "|07 PM|17031839100|17031839100|   192|     1.06|    6.85|\n",
      "|07 PM|17031839100|17031320100|   133|     1.09|    7.82|\n",
      "|07 PM|17031839100|17031081700|   129|     1.24|    7.84|\n",
      "|07 PM|17031839100|17031081500|   124|     1.23|    8.64|\n",
      "|07 PM|17031081500|17031839100|   114|     1.21|     8.7|\n",
      "|08 AM|17031281900|17031839100|   349|     0.73|    6.56|\n",
      "|08 AM|17031320100|17031839100|   336|      0.9|    7.08|\n",
      "|08 AM|17031839100|17031839100|   284|      0.8|    6.72|\n",
      "|08 AM|17031280100|17031839100|   215|     0.77|    6.68|\n",
      "|08 AM|17031081500|17031839100|   166|     1.16|    8.69|\n",
      "|08 PM|17031839100|17031839100|   105|     0.85|     6.4|\n",
      "|08 PM|17031980000|17031081500|    83|    16.62|    50.6|\n",
      "|08 PM|17031839100|17031081700|    83|     0.89|     7.5|\n",
      "|08 PM|17031839100|17031081403|    82|     1.26|    8.27|\n",
      "|08 PM|17031980000|17031839100|    79|    14.46|   51.47|\n",
      "|09 AM|17031281900|17031839100|   311|     0.72|    6.77|\n",
      "|09 AM|17031320100|17031839100|   300|     0.96|    7.29|\n",
      "|09 AM|17031839100|17031839100|   296|     0.82|    7.13|\n",
      "|09 AM|17031081500|17031839100|   182|     1.22|    9.04|\n",
      "|09 AM|17031280100|17031839100|   170|     0.75|    6.86|\n",
      "|09 PM|17031980000|17031320100|    85|    36.31|    52.7|\n",
      "|09 PM|17031980000|17031839100|    72|    16.44|   51.32|\n",
      "|09 PM|17031081700|17031320100|    70|     0.94|     7.5|\n",
      "|09 PM|17031839100|17031839100|    69|     1.25|    7.57|\n",
      "|09 PM|17031839100|17031081700|    69|     0.93|    7.43|\n",
      "|10 AM|17031839100|17031839100|   267|     0.87|    6.93|\n",
      "|10 AM|17031320100|17031839100|   201|     1.07|    7.74|\n",
      "|10 AM|17031281900|17031839100|   172|      0.8|    6.84|\n",
      "|10 AM|17031839100|17031320100|   139|     0.94|    7.36|\n",
      "|10 AM|17031980000|17031839100|   131|    28.32|    52.4|\n",
      "|10 PM|17031839100|17031320100|    72|     0.97|    7.37|\n",
      "|10 PM|17031980000|17031839100|    65|    16.52|   50.12|\n",
      "|10 PM|17031839100|17031839100|    62|     1.02|    7.19|\n",
      "|10 PM|17031839100|17031281900|    56|     0.75|    6.34|\n",
      "|10 PM|17031839100|17031081500|    55|     1.26|    7.86|\n",
      "|11 AM|17031839100|17031839100|   338|     0.87|    7.25|\n",
      "|11 AM|17031839100|17031320100|   203|      0.9|    7.96|\n",
      "|11 AM|17031320100|17031839100|   176|     1.04|    7.54|\n",
      "|11 AM|17031839100|17031081700|   151|     0.88|    7.37|\n",
      "|11 AM|17031839100|17031081500|   139|     1.13|     7.8|\n",
      "|11 PM|17031839100|17031081700|    53|     0.94|    7.67|\n",
      "|11 PM|17031081700|17031081700|    47|      0.8|    7.46|\n",
      "|11 PM|17031081700|17031839100|    46|      0.8|    7.15|\n",
      "|11 PM|17031081700|17031081800|    46|     0.75|    6.94|\n",
      "|11 PM|17031839100|17031320100|    44|      1.0|    7.26|\n",
      "|12 AM|17031081700|17031081700|    62|     0.57|    6.87|\n",
      "|12 AM|17031081700|17031081800|    50|     0.65|    6.78|\n",
      "|12 AM|17031081700|17031320100|    40|     0.91|    7.26|\n",
      "|12 AM|17031081800|17031081700|    39|     0.48|    8.71|\n",
      "|12 AM|17031980000|17031081500|    27|     14.4|   51.66|\n",
      "|12 PM|17031839100|17031839100|   331|     1.05|    7.48|\n",
      "|12 PM|17031839100|17031320100|   187|     1.01|    7.61|\n",
      "|12 PM|17031320100|17031839100|   182|     0.89|    7.47|\n",
      "|12 PM|17031839100|17031081700|   161|     0.85|    7.49|\n",
      "|12 PM|17031081500|17031839100|   131|     1.32|    7.96|\n",
      "+-----+-----------+-----------+------+---------+--------+\n",
      "\n",
      "CPU times: user 168 ms, sys: 90.5 ms, total: 259 ms\n",
      "Wall time: 20.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('dataframes_exercise3a').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try :\n",
    "    lines = sc.textFile('Taxi_small.csv')\n",
    "    logRows = lines.filter(lambda line : len(line) > 0 ) \\\n",
    "        .map(lambda line : line.strip()) \\\n",
    "        .map(lambda line : line.split(';')) \\\n",
    "        .filter(lambda arr: arr[2] != '' and arr[5] != '' and arr[5]!='0' and arr[6] != '' and arr[7] != '' ) \\\n",
    "        .map(lambda arr: arr[:10]+list([float(arr[10].replace(',',''))])+arr[11:] if (arr[10]!='') else (arr[:10]+list([0])+arr[11:]))\\\n",
    "        .map(lambda arr: arr[:11]+list([float(arr[11].replace(',',''))])+arr[12:] if (arr[11]!='') else (arr[:11]+list([0])+arr[12:]))\\\n",
    "        .map(lambda arr: arr[:12]+list([float(arr[12].replace(',',''))])+arr[13:] if (arr[12]!='') else (arr[:12]+list([0])+arr[13:]))\\\n",
    "        .map(lambda arr: arr[:13]+list([float(arr[13].replace(',',''))])+arr[14:] if (arr[13]!='') else (arr[:13]+list([0])+arr[14:]))\\\n",
    "        .map(lambda arr: arr[:14]+list([float(arr[14].replace(',',''))])+arr[15:] if (arr[14]!='') else (arr[:14]+list([0])+arr[15:]))\\\n",
    "        .filter(lambda arr: arr[10]!=0 or arr[11]!=0 or arr[12]!=0 or arr[13]!=0 or arr[14]!=0) \\\n",
    "        .map(lambda arr: arr if (arr[10]+ arr[11]+ arr[12]+arr[13]==arr[14] )\\\n",
    "                             else (arr[:14]+[arr[10]+ arr[11]+ arr[12]+arr[13]]+arr[15:]))\\\n",
    "        .map( lambda arr : Row(hour_location = (arr[2][11:][:2] + \" \" + arr[2][11:][9:] + \"_\" + arr[6] + \"_\" + arr[7]),\n",
    "                                miles = float(arr[5].replace(',','')), cost= arr[14]))\n",
    "    \n",
    "    logRowsDF = spark.createDataFrame( logRows )\n",
    "\n",
    "    \n",
    "    route = logRowsDF.groupBy('hour_location').agg(count('hour_location').alias('#trips'),\n",
    "        round(avg('miles'),2).alias('avg_miles'),\n",
    "        round(avg('cost'),2).alias('avg_cost')).orderBy('hour_location','#trips', ascending=False)\n",
    "    \n",
    "    \n",
    "    route = route.select(split(col('hour_location'),'_').getItem(0).alias('hour'),\n",
    "                         split(col('hour_location'),'_').getItem(1).alias('Pick_up'),\n",
    "                         split(col('hour_location'),'_').getItem(2).alias('Drop_off'),\n",
    "                         '#trips','avg_miles','avg_cost')\n",
    "   \n",
    "\n",
    "    \n",
    "    window = Window.partitionBy(route['hour']).orderBy(route['#trips'].desc())\n",
    "    \n",
    "    \n",
    "    top = route.select('*',row_number().over(window).alias('rank')).filter(col('rank')<=5)\n",
    "    top.orderBy('hour','rank').drop('rank').show(120)\n",
    "    \n",
    "    \n",
    "    \n",
    "except Exception as err:\n",
    "    print(err)\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise version 3B - pre-processing after DataFrame creation and concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----------+------+---------+--------+\n",
      "| hour|    Pick_up|   Drop_off|#trips|avg_miles|avg_cost|\n",
      "+-----+-----------+-----------+------+---------+--------+\n",
      "|01 AM|17031081700|17031081700|    63|      0.7|    7.15|\n",
      "|01 AM|17031081700|17031081800|    56|     0.71|    6.94|\n",
      "|01 AM|17031081700|17031839100|    35|     0.92|     7.9|\n",
      "|01 AM|17031081700|17031320100|    34|     0.94|     7.6|\n",
      "|01 AM|17031081800|17031081800|    32|     0.62|    5.97|\n",
      "|01 PM|17031839100|17031839100|   316|     0.74|    6.83|\n",
      "|01 PM|17031320100|17031839100|   219|     0.91|    7.53|\n",
      "|01 PM|17031839100|17031320100|   215|     0.98|    7.48|\n",
      "|01 PM|17031081500|17031839100|   144|      1.1|    8.01|\n",
      "|01 PM|17031081700|17031839100|   136|     0.76|    7.13|\n",
      "|02 AM|17031081700|17031081800|    53|     0.62|    6.63|\n",
      "|02 AM|17031081700|17031081700|    44|     0.74|    6.91|\n",
      "|02 AM|17031081700|17031320100|    35|     0.95|    7.75|\n",
      "|02 AM|17031081800|17031081800|    27|     1.15|     8.4|\n",
      "|02 AM|17031081800|17031081700|    27|     0.55|    7.56|\n",
      "|02 PM|17031839100|17031839100|   332|     0.87|    7.11|\n",
      "|02 PM|17031839100|17031320100|   184|     0.93|     7.3|\n",
      "|02 PM|17031320100|17031839100|   170|     4.36|    7.82|\n",
      "|02 PM|17031839100|17031980000|   154|    16.16|   48.77|\n",
      "|02 PM|17031081500|17031839100|   152|     1.37|     8.4|\n",
      "|03 AM|17031081700|17031081800|    37|     0.78|    7.23|\n",
      "|03 AM|17031081700|17031081700|    25|     1.18|    8.39|\n",
      "|03 AM|17031081800|17031081700|    21|     0.89|     6.7|\n",
      "|03 AM|17031081700|17031839100|    20|     0.81|    7.06|\n",
      "|03 AM|17031081800|17031081800|    17|     0.85|     7.2|\n",
      "|03 PM|17031839100|17031839100|   310|     0.92|    6.67|\n",
      "|03 PM|17031320100|17031839100|   197|      1.0|    7.65|\n",
      "|03 PM|17031839100|17031320100|   195|     0.87|    6.91|\n",
      "|03 PM|17031839100|17031281900|   185|     0.82|    6.13|\n",
      "|03 PM|17031839100|17031980000|   163|     15.5|   50.61|\n",
      "|04 AM|17031081700|17031081800|    25|     0.55|    6.35|\n",
      "|04 AM|17031839100|17031980000|    19|    16.84|   45.89|\n",
      "|04 AM|17031081500|17031980000|    16|    17.93|    46.2|\n",
      "|04 AM|17031081403|17031980000|    12|     17.7|   45.09|\n",
      "|04 AM|17031081800|17031081800|    11|     0.45|    5.76|\n",
      "|04 PM|17031839100|17031839100|   366|     0.76|    6.51|\n",
      "|04 PM|17031839100|17031281900|   260|     0.75|    6.23|\n",
      "|04 PM|17031839100|17031320100|   237|     0.84|    7.04|\n",
      "|04 PM|17031320100|17031839100|   199|     0.99|    7.72|\n",
      "|04 PM|17031839100|17031081500|   142|     1.25|    8.59|\n",
      "|05 AM|17031320100|17031980000|    59|    17.03|   50.14|\n",
      "|05 AM|17031081500|17031980000|    42|     15.3|   48.51|\n",
      "|05 AM|17031839100|17031980000|    32|     17.9|   48.38|\n",
      "|05 AM|17031081401|17031980000|    19|    16.97|   48.69|\n",
      "|05 AM|17031980000|17031980000|    17|    10.71|   31.08|\n",
      "|05 PM|17031839100|17031839100|   329|     0.82|    6.96|\n",
      "|05 PM|17031839100|17031281900|   264|     0.72|    6.54|\n",
      "|05 PM|17031839100|17031320100|   228|     0.89|    7.28|\n",
      "|05 PM|17031320100|17031839100|   164|     0.98|    8.11|\n",
      "|05 PM|17031839100|17031280100|   157|     0.85|    6.92|\n",
      "|06 AM|17031320100|17031980000|    68|    19.28|   49.26|\n",
      "|06 AM|17031081500|17031980000|    64|    16.64|   46.96|\n",
      "|06 AM|17031320100|17031839100|    56|      1.0|    6.34|\n",
      "|06 AM|17031839100|17031980000|    54|     16.6|   50.01|\n",
      "|06 AM|17031281900|17031839100|    50|     0.77|    5.64|\n",
      "|06 PM|17031839100|17031839100|   252|     0.73|    6.72|\n",
      "|06 PM|17031839100|17031320100|   188|     1.16|    7.67|\n",
      "|06 PM|17031839100|17031081500|   186|     1.18|    8.75|\n",
      "|06 PM|17031839100|17031081700|   181|     0.95|    8.08|\n",
      "|06 PM|17031839100|17031281900|   169|     0.65|    6.35|\n",
      "|07 AM|17031281900|17031839100|   183|     0.76|    5.98|\n",
      "|07 AM|17031320100|17031839100|   175|     0.92|    6.58|\n",
      "|07 AM|17031839100|17031839100|   152|      0.7|    6.17|\n",
      "|07 AM|17031280100|17031839100|   145|     0.73|    6.08|\n",
      "|07 AM|17031081500|17031839100|   122|      1.3|    8.49|\n",
      "|07 PM|17031839100|17031839100|   192|     1.06|    6.85|\n",
      "|07 PM|17031839100|17031320100|   133|     1.09|    7.82|\n",
      "|07 PM|17031839100|17031081700|   129|     1.24|    7.84|\n",
      "|07 PM|17031839100|17031081500|   124|     1.23|    8.64|\n",
      "|07 PM|17031081500|17031839100|   114|     1.21|     8.7|\n",
      "|08 AM|17031281900|17031839100|   349|     0.73|    6.56|\n",
      "|08 AM|17031320100|17031839100|   336|      0.9|    7.08|\n",
      "|08 AM|17031839100|17031839100|   284|      0.8|    6.72|\n",
      "|08 AM|17031280100|17031839100|   215|     0.77|    6.68|\n",
      "|08 AM|17031081500|17031839100|   166|     1.16|    8.69|\n",
      "|08 PM|17031839100|17031839100|   105|     0.85|     6.4|\n",
      "|08 PM|17031980000|17031081500|    83|    16.62|    50.6|\n",
      "|08 PM|17031839100|17031081700|    83|     0.89|     7.5|\n",
      "|08 PM|17031839100|17031081403|    82|     1.26|    8.27|\n",
      "|08 PM|17031980000|17031839100|    79|    14.46|   51.47|\n",
      "|09 AM|17031281900|17031839100|   311|     0.72|    6.77|\n",
      "|09 AM|17031320100|17031839100|   300|     0.96|    7.29|\n",
      "|09 AM|17031839100|17031839100|   296|     0.82|    7.13|\n",
      "|09 AM|17031081500|17031839100|   182|     1.22|    9.04|\n",
      "|09 AM|17031280100|17031839100|   170|     0.75|    6.86|\n",
      "|09 PM|17031980000|17031320100|    85|    36.31|    52.7|\n",
      "|09 PM|17031980000|17031839100|    72|    16.44|   51.32|\n",
      "|09 PM|17031081700|17031320100|    70|     0.94|     7.5|\n",
      "|09 PM|17031839100|17031839100|    69|     1.25|    7.57|\n",
      "|09 PM|17031839100|17031081700|    69|     0.93|    7.43|\n",
      "|10 AM|17031839100|17031839100|   267|     0.87|    6.93|\n",
      "|10 AM|17031320100|17031839100|   201|     1.07|    7.74|\n",
      "|10 AM|17031281900|17031839100|   172|      0.8|    6.84|\n",
      "|10 AM|17031839100|17031320100|   139|     0.94|    7.36|\n",
      "|10 AM|17031980000|17031839100|   131|    28.32|    52.4|\n",
      "|10 PM|17031839100|17031320100|    72|     0.97|    7.37|\n",
      "|10 PM|17031980000|17031839100|    65|    16.52|   50.12|\n",
      "|10 PM|17031839100|17031839100|    62|     1.02|    7.19|\n",
      "|10 PM|17031839100|17031281900|    56|     0.75|    6.34|\n",
      "|10 PM|17031839100|17031081500|    55|     1.26|    7.86|\n",
      "|11 AM|17031839100|17031839100|   338|     0.87|    7.25|\n",
      "|11 AM|17031839100|17031320100|   203|      0.9|    7.96|\n",
      "|11 AM|17031320100|17031839100|   176|     1.04|    7.54|\n",
      "|11 AM|17031839100|17031081700|   151|     0.88|    7.37|\n",
      "|11 AM|17031839100|17031081500|   139|     1.13|     7.8|\n",
      "|11 PM|17031839100|17031081700|    53|     0.94|    7.67|\n",
      "|11 PM|17031081700|17031081700|    47|      0.8|    7.46|\n",
      "|11 PM|17031081700|17031839100|    46|      0.8|    7.15|\n",
      "|11 PM|17031081700|17031081800|    46|     0.75|    6.94|\n",
      "|11 PM|17031839100|17031320100|    44|      1.0|    7.26|\n",
      "|12 AM|17031081700|17031081700|    62|     0.57|    6.87|\n",
      "|12 AM|17031081700|17031081800|    50|     0.65|    6.78|\n",
      "|12 AM|17031081700|17031320100|    40|     0.91|    7.26|\n",
      "|12 AM|17031081800|17031081700|    39|     0.48|    8.71|\n",
      "|12 AM|17031980000|17031081500|    27|     14.4|   51.66|\n",
      "|12 PM|17031839100|17031839100|   331|     1.05|    7.48|\n",
      "|12 PM|17031839100|17031320100|   187|     1.01|    7.61|\n",
      "|12 PM|17031320100|17031839100|   182|     0.89|    7.47|\n",
      "|12 PM|17031839100|17031081700|   161|     0.85|    7.49|\n",
      "|12 PM|17031081500|17031839100|   131|     1.32|    7.96|\n",
      "+-----+-----------+-----------+------+---------+--------+\n",
      "\n",
      "CPU times: user 226 ms, sys: 121 ms, total: 347 ms\n",
      "Wall time: 20.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('dataframes_exercise3b').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try :\n",
    "    lines = sc.textFile('Taxi_small.csv')\n",
    "    logRows = lines.filter(lambda line : len(line) > 0 ) \\\n",
    "        .map(lambda line : line.strip()) \\\n",
    "        .map(lambda line : line.split(';')) \\\n",
    "        .filter(lambda arr: arr[2] != '' and arr[5] != '' and arr[6] != '' and arr[7] != '' ) \\\n",
    "        .map( lambda arr : Row(hour_location = (arr[2][11:][:2] + \" \" + arr[2][11:][9:] + \"_\" + arr[6] + \"_\" + arr[7]),\n",
    "                                miles = arr[5], cost= arr[14],Fare = arr[10], Tips = arr[11], \\\n",
    "                               Tolls = arr[12], Extras = arr[13] ))\n",
    "    \n",
    "    tripRowsDF = spark.createDataFrame( logRows ) \n",
    "    \n",
    "    \n",
    "    processed=tripRowsDF.where((col('miles')!='')& (col('miles')!='0'))\n",
    "    processed=processed.withColumn(\"Miles\",regexp_replace(col(\"miles\"), \",\", \"\")).drop(processed.miles)\n",
    "    processed=processed.withColumn('fare',when(processed.Fare !='',regexp_replace(col(\"Fare\"), \",\", \"\")).otherwise(0))\\\n",
    "    .drop(processed.Fare)\\\n",
    "    .withColumn('tips',when(processed.Tips !='',regexp_replace(col(\"Tips\"), \",\", \"\")).otherwise(0))\\\n",
    "    .drop(processed.Tips)\\\n",
    "    .withColumn('tolls',when(processed.Tolls !='',regexp_replace(col(\"Tolls\"), \",\", \"\")).otherwise(0))\\\n",
    "    .drop(processed.Tolls)\\\n",
    "    .withColumn('extras',when(processed.Extras !='',regexp_replace(col(\"Extras\"), \",\", \"\")).otherwise(0))\\\n",
    "    .drop(processed.Extras)\\\n",
    "    .withColumn('t_cost',when(processed.cost !='',regexp_replace(col(\"cost\"), \",\", \"\")).otherwise(0))\\\n",
    "    .drop(processed.cost)\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    processed=processed.where((col('fare')!=0) | (col('tips')!=0) | (col('tolls')!=0) | (col('extras')!=0) | (col('t_cost')!=0))\n",
    "    \n",
    "    \n",
    "    processed=processed.withColumn('total_cost',when(processed.fare+processed.tips+processed.tolls+processed.extras \\\n",
    "                                  !=processed.t_cost,processed.fare+processed.tips+processed.tolls+processed.extras)\\\n",
    "                                   .otherwise(processed.t_cost))\\\n",
    "    .drop(processed.t_cost)\n",
    "    \n",
    "    \n",
    "    \n",
    "    route = processed.groupBy('hour_location').agg(count('hour_location').alias('#trips'),\n",
    "        round(avg('miles'),2).alias('avg_miles'),\n",
    "        round(avg('total_cost'),2).alias('avg_cost')).orderBy('hour_location','#trips', ascending=False)\n",
    "    \n",
    "    \n",
    "    route = route.select(split(col('hour_location'),'_').getItem(0).alias('hour'),\n",
    "                         split(col('hour_location'),'_').getItem(1).alias('Pick_up'),\n",
    "                         split(col('hour_location'),'_').getItem(2).alias('Drop_off'),\n",
    "                         '#trips','avg_miles','avg_cost')\n",
    "   \n",
    "    route=route.where((col('hour')!='')& (col('Pick_up')!='0') & (col('Drop_off')!='0'))\n",
    "\n",
    "    \n",
    "    window = Window.partitionBy(route['hour']).orderBy(route['#trips'].desc())\n",
    "    \n",
    "    \n",
    "    top = route.select('*',row_number().over(window).alias('rank')).filter(col('rank')<=5)\n",
    "    top.orderBy('hour','rank').drop('rank').show(120)\n",
    "    \n",
    "    \n",
    "    \n",
    "except Exception as err:\n",
    "    print(err)\n",
    "    sc.stop()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise version 3C - pre-processing after DataFrame creation and no concatenation before aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----------+------+---------+--------+\n",
      "| hour|     pickup|    dropoff|#trips|avg_miles|avg_cost|\n",
      "+-----+-----------+-----------+------+---------+--------+\n",
      "|01 AM|17031081700|17031081700|    63|      0.7|    7.15|\n",
      "|01 AM|17031081700|17031081800|    56|     0.71|    6.94|\n",
      "|01 AM|17031081700|17031839100|    35|     0.92|     7.9|\n",
      "|01 AM|17031081700|17031320100|    34|     0.94|     7.6|\n",
      "|01 AM|17031081800|17031081800|    32|     0.62|    5.97|\n",
      "|01 PM|17031839100|17031839100|   316|     0.74|    6.83|\n",
      "|01 PM|17031320100|17031839100|   219|     0.91|    7.53|\n",
      "|01 PM|17031839100|17031320100|   215|     0.98|    7.48|\n",
      "|01 PM|17031081500|17031839100|   144|      1.1|    8.01|\n",
      "|01 PM|17031081700|17031839100|   136|     0.76|    7.13|\n",
      "|02 AM|17031081700|17031081800|    53|     0.62|    6.63|\n",
      "|02 AM|17031081700|17031081700|    44|     0.74|    6.91|\n",
      "|02 AM|17031081700|17031320100|    35|     0.95|    7.75|\n",
      "|02 AM|17031081800|17031081800|    27|     1.15|     8.4|\n",
      "|02 AM|17031081800|17031081700|    27|     0.55|    7.56|\n",
      "|02 PM|17031839100|17031839100|   332|     0.87|    7.11|\n",
      "|02 PM|17031839100|17031320100|   184|     0.93|     7.3|\n",
      "|02 PM|17031320100|17031839100|   170|     4.36|    7.82|\n",
      "|02 PM|17031839100|17031980000|   154|    16.16|   48.77|\n",
      "|02 PM|17031081500|17031839100|   152|     1.37|     8.4|\n",
      "|03 AM|17031081700|17031081800|    37|     0.78|    7.23|\n",
      "|03 AM|17031081700|17031081700|    25|     1.18|    8.39|\n",
      "|03 AM|17031081800|17031081700|    21|     0.89|     6.7|\n",
      "|03 AM|17031081700|17031839100|    20|     0.81|    7.06|\n",
      "|03 AM|17031081800|17031081800|    17|     0.85|     7.2|\n",
      "|03 PM|17031839100|17031839100|   310|     0.92|    6.67|\n",
      "|03 PM|17031320100|17031839100|   197|      1.0|    7.65|\n",
      "|03 PM|17031839100|17031320100|   195|     0.87|    6.91|\n",
      "|03 PM|17031839100|17031281900|   185|     0.82|    6.13|\n",
      "|03 PM|17031839100|17031980000|   163|     15.5|   50.61|\n",
      "|04 AM|17031081700|17031081800|    25|     0.55|    6.35|\n",
      "|04 AM|17031839100|17031980000|    19|    16.84|   45.89|\n",
      "|04 AM|17031081500|17031980000|    16|    17.93|    46.2|\n",
      "|04 AM|17031081403|17031980000|    12|     17.7|   45.09|\n",
      "|04 AM|17031081800|17031081800|    11|     0.45|    5.76|\n",
      "|04 PM|17031839100|17031839100|   366|     0.76|    6.51|\n",
      "|04 PM|17031839100|17031281900|   260|     0.75|    6.23|\n",
      "|04 PM|17031839100|17031320100|   237|     0.84|    7.04|\n",
      "|04 PM|17031320100|17031839100|   199|     0.99|    7.72|\n",
      "|04 PM|17031839100|17031081500|   142|     1.25|    8.59|\n",
      "|05 AM|17031320100|17031980000|    59|    17.03|   50.14|\n",
      "|05 AM|17031081500|17031980000|    42|     15.3|   48.51|\n",
      "|05 AM|17031839100|17031980000|    32|     17.9|   48.38|\n",
      "|05 AM|17031081401|17031980000|    19|    16.97|   48.69|\n",
      "|05 AM|17031980000|17031980000|    17|    10.71|   31.08|\n",
      "|05 PM|17031839100|17031839100|   329|     0.82|    6.96|\n",
      "|05 PM|17031839100|17031281900|   264|     0.72|    6.54|\n",
      "|05 PM|17031839100|17031320100|   228|     0.89|    7.28|\n",
      "|05 PM|17031320100|17031839100|   164|     0.98|    8.11|\n",
      "|05 PM|17031839100|17031280100|   157|     0.85|    6.92|\n",
      "|06 AM|17031320100|17031980000|    68|    19.28|   49.26|\n",
      "|06 AM|17031081500|17031980000|    64|    16.64|   46.96|\n",
      "|06 AM|17031320100|17031839100|    56|      1.0|    6.34|\n",
      "|06 AM|17031839100|17031980000|    54|     16.6|   50.01|\n",
      "|06 AM|17031281900|17031839100|    50|     0.77|    5.64|\n",
      "|06 PM|17031839100|17031839100|   252|     0.73|    6.72|\n",
      "|06 PM|17031839100|17031320100|   188|     1.16|    7.67|\n",
      "|06 PM|17031839100|17031081500|   186|     1.18|    8.75|\n",
      "|06 PM|17031839100|17031081700|   181|     0.95|    8.08|\n",
      "|06 PM|17031839100|17031281900|   169|     0.65|    6.35|\n",
      "|07 AM|17031281900|17031839100|   183|     0.76|    5.98|\n",
      "|07 AM|17031320100|17031839100|   175|     0.92|    6.58|\n",
      "|07 AM|17031839100|17031839100|   152|      0.7|    6.17|\n",
      "|07 AM|17031280100|17031839100|   145|     0.73|    6.08|\n",
      "|07 AM|17031081500|17031839100|   122|      1.3|    8.49|\n",
      "|07 PM|17031839100|17031839100|   192|     1.06|    6.85|\n",
      "|07 PM|17031839100|17031320100|   133|     1.09|    7.82|\n",
      "|07 PM|17031839100|17031081700|   129|     1.24|    7.84|\n",
      "|07 PM|17031839100|17031081500|   124|     1.23|    8.64|\n",
      "|07 PM|17031081500|17031839100|   114|     1.21|     8.7|\n",
      "|08 AM|17031281900|17031839100|   349|     0.73|    6.56|\n",
      "|08 AM|17031320100|17031839100|   336|      0.9|    7.08|\n",
      "|08 AM|17031839100|17031839100|   284|      0.8|    6.72|\n",
      "|08 AM|17031280100|17031839100|   215|     0.77|    6.68|\n",
      "|08 AM|17031081500|17031839100|   166|     1.16|    8.69|\n",
      "|08 PM|17031839100|17031839100|   105|     0.85|     6.4|\n",
      "|08 PM|17031980000|17031081500|    83|    16.62|    50.6|\n",
      "|08 PM|17031839100|17031081700|    83|     0.89|     7.5|\n",
      "|08 PM|17031839100|17031081403|    82|     1.26|    8.27|\n",
      "|08 PM|17031980000|17031839100|    79|    14.46|   51.47|\n",
      "|09 AM|17031281900|17031839100|   311|     0.72|    6.77|\n",
      "|09 AM|17031320100|17031839100|   300|     0.96|    7.29|\n",
      "|09 AM|17031839100|17031839100|   296|     0.82|    7.13|\n",
      "|09 AM|17031081500|17031839100|   182|     1.22|    9.04|\n",
      "|09 AM|17031280100|17031839100|   170|     0.75|    6.86|\n",
      "|09 PM|17031980000|17031320100|    85|    36.31|    52.7|\n",
      "|09 PM|17031980000|17031839100|    72|    16.44|   51.32|\n",
      "|09 PM|17031081700|17031320100|    70|     0.94|     7.5|\n",
      "|09 PM|17031839100|17031839100|    69|     1.25|    7.57|\n",
      "|09 PM|17031839100|17031081700|    69|     0.93|    7.43|\n",
      "|10 AM|17031839100|17031839100|   267|     0.87|    6.93|\n",
      "|10 AM|17031320100|17031839100|   201|     1.07|    7.74|\n",
      "|10 AM|17031281900|17031839100|   172|      0.8|    6.84|\n",
      "|10 AM|17031839100|17031320100|   139|     0.94|    7.36|\n",
      "|10 AM|17031980000|17031839100|   131|    28.32|    52.4|\n",
      "|10 PM|17031839100|17031320100|    72|     0.97|    7.37|\n",
      "|10 PM|17031980000|17031839100|    65|    16.52|   50.12|\n",
      "|10 PM|17031839100|17031839100|    62|     1.02|    7.19|\n",
      "|10 PM|17031839100|17031281900|    56|     0.75|    6.34|\n",
      "|10 PM|17031839100|17031081500|    55|     1.26|    7.86|\n",
      "|11 AM|17031839100|17031839100|   338|     0.87|    7.25|\n",
      "|11 AM|17031839100|17031320100|   203|      0.9|    7.96|\n",
      "|11 AM|17031320100|17031839100|   176|     1.04|    7.54|\n",
      "|11 AM|17031839100|17031081700|   151|     0.88|    7.37|\n",
      "|11 AM|17031839100|17031081500|   139|     1.13|     7.8|\n",
      "|11 PM|17031839100|17031081700|    53|     0.94|    7.67|\n",
      "|11 PM|17031081700|17031081700|    47|      0.8|    7.46|\n",
      "|11 PM|17031081700|17031839100|    46|      0.8|    7.15|\n",
      "|11 PM|17031081700|17031081800|    46|     0.75|    6.94|\n",
      "|11 PM|17031839100|17031320100|    44|      1.0|    7.26|\n",
      "|12 AM|17031081700|17031081700|    62|     0.57|    6.87|\n",
      "|12 AM|17031081700|17031081800|    50|     0.65|    6.78|\n",
      "|12 AM|17031081700|17031320100|    40|     0.91|    7.26|\n",
      "|12 AM|17031081800|17031081700|    39|     0.48|    8.71|\n",
      "|12 AM|17031980000|17031081500|    27|     14.4|   51.66|\n",
      "|12 PM|17031839100|17031839100|   331|     1.05|    7.48|\n",
      "|12 PM|17031839100|17031320100|   187|     1.01|    7.61|\n",
      "|12 PM|17031320100|17031839100|   182|     0.89|    7.47|\n",
      "|12 PM|17031839100|17031081700|   161|     0.85|    7.49|\n",
      "|12 PM|17031081500|17031839100|   131|     1.32|    7.96|\n",
      "+-----+-----------+-----------+------+---------+--------+\n",
      "\n",
      "CPU times: user 246 ms, sys: 134 ms, total: 380 ms\n",
      "Wall time: 19.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('dataframes_exercise3c').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try :\n",
    "    lines = sc.textFile('Taxi_small.csv')\n",
    "    logRows = lines.filter(lambda line : len(line) > 0 ) \\\n",
    "        .map(lambda line : line.strip()) \\\n",
    "        .map(lambda line : line.split(';')) \\\n",
    "        .filter(lambda arr: arr[2] != '' and arr[5] != '' and arr[6] != '' and arr[7] != '' ) \\\n",
    "        .map( lambda arr : Row(hour = (arr[2][11:][:2] + \" \" + arr[2][11:][9:]),pickup= arr[6], dropoff=arr[7],\n",
    "                                miles = arr[5], cost= arr[14],Fare = arr[10], Tips = arr[11], \\\n",
    "                               Tolls = arr[12], Extras = arr[13] ))\n",
    "    \n",
    "    tripRowsDF = spark.createDataFrame( logRows ) \n",
    "    \n",
    "    \n",
    "    processed=tripRowsDF.where((col('miles')!='')& (col('miles')!='0') & (col('hour')!='')& (col('pickup')!='')&\\\n",
    "                               (col('dropoff')!=''))\n",
    "    processed=processed.withColumn(\"Miles\",regexp_replace(col(\"miles\"), \",\", \"\")).drop(processed.miles)\n",
    "   \n",
    "    processed=processed.withColumn('fare',when(processed.Fare !='',regexp_replace(col(\"Fare\"), \",\", \"\")).otherwise(0))\\\n",
    "    .drop(processed.Fare)\\\n",
    "    .withColumn('tips',when(processed.Tips !='',regexp_replace(col(\"Tips\"), \",\", \"\")).otherwise(0))\\\n",
    "    .drop(processed.Tips)\\\n",
    "    .withColumn('tolls',when(processed.Tolls !='',regexp_replace(col(\"Tolls\"), \",\", \"\")).otherwise(0))\\\n",
    "    .drop(processed.Tolls)\\\n",
    "    .withColumn('extras',when(processed.Extras !='',regexp_replace(col(\"Extras\"), \",\", \"\")).otherwise(0))\\\n",
    "    .drop(processed.Extras)\\\n",
    "    .withColumn('t_cost',when(processed.cost !='',regexp_replace(col(\"cost\"), \",\", \"\")).otherwise(0))\\\n",
    "    .drop(processed.cost)\n",
    "    \n",
    "    processed=processed.where((col('fare')!=0) | (col('tips')!=0) | (col('tolls')!=0) | (col('extras')!=0) | (col('t_cost')!=0))\n",
    "    \n",
    "    \n",
    "    processed=processed.withColumn('total_cost',when(processed.fare+processed.tips+processed.tolls+processed.extras \\\n",
    "                                  !=processed.t_cost,processed.fare+processed.tips+processed.tolls+processed.extras)\\\n",
    "                                   .otherwise(processed.t_cost))\\\n",
    "    .drop(processed.t_cost)\n",
    "    \n",
    "    \n",
    "    \n",
    "    route = processed.groupBy('hour','pickup','dropoff').agg(count('hour').alias('#trips'),\n",
    "        round(avg('miles'),2).alias('avg_miles'),\n",
    "        round(avg('total_cost'),2).alias('avg_cost')).orderBy('hour','pickup','dropoff','#trips', ascending=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    window = Window.partitionBy(route['hour']).orderBy(route['#trips'].desc())\n",
    "    \n",
    "    \n",
    "    top = route.select('*',row_number().over(window).alias('rank')).filter(col('rank')<=5)\n",
    "    top.orderBy('hour','rank').drop('rank').show(120)\n",
    "    \n",
    "    \n",
    "    \n",
    "except Exception as err:\n",
    "    print(err)\n",
    "    sc.stop()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Caculate average and maximum tip for each year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----------+\n",
      "|Year|Avg_tips|Biggest_tip|\n",
      "+----+--------+-----------+\n",
      "|2013|    0.96|       99.0|\n",
      "|2014|    1.12|      120.0|\n",
      "|2015|    1.37|      100.0|\n",
      "|2016|    1.49|       60.0|\n",
      "|2017|    1.55|      261.0|\n",
      "|2018|    1.72|       90.0|\n",
      "|2019|    1.86|       59.0|\n",
      "|2020|     1.5|       30.0|\n",
      "+----+--------+-----------+\n",
      "\n",
      "CPU times: user 86.4 ms, sys: 20 ms, total: 106 ms\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.master('local[*]').appName('dataframes_exercise4').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try :\n",
    "    lines = sc.textFile('Taxi_small.csv')\n",
    "    logRows = lines.filter( lambda line : len(line) > 0 )   \\\n",
    "                        .map( lambda line : line.split(';') ) \\\n",
    "                        .filter(lambda arr: arr[2] != ''and arr[11] != '') \\\n",
    "                        .map( lambda arr : Row(Trip_ID = arr[0], Taxi_ID = arr[1], Year = arr[2].split(\" \")[0].split(\"/\")[2], \\\n",
    "                                               Trip_End = arr[3], Trip_Seconds = arr[4], Trip_Miles = arr[5],\\\n",
    "                                               Pickup_ID = arr[6], Dropoff_ID = arr[7], Pickup_area =  arr[8], \\\n",
    "                                               Dropoff_area = arr[9], Fare = arr[10], Tips = float(arr[11].replace(' , ', '')), Tolls = arr[12], \\\n",
    "                                               Extras = arr[13], Trip_Total= arr[14],Payment_type = arr[15] , \\\n",
    "                                               Company = arr[16], Pickup_Centroid_Lat  = arr[17], \\\n",
    "                                               Pickup_Centroid_Lon = arr[18], Pickup_Centroid_Loc = arr[19], \\\n",
    "                                               Dropoff_Centroid_Lat = arr[20], Dropoff_Centroid_Lon = arr[21], \\\n",
    "                                               Dropoff_Centroid_Lo = arr[22] ))\n",
    "    \n",
    "    logRowsDF = spark.createDataFrame( logRows )\n",
    "    \n",
    "    \n",
    "    tip = logRowsDF.groupBy(\"Year\").agg(round(avg('Tips'),2).alias('Avg_tips'),\n",
    "        round(max(\"Tips\"),2).alias('Biggest_tip'))\n",
    "    \n",
    "    \n",
    "    tip.orderBy(\"Year\").show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "except Exception as err:\n",
    "    print(err)\n",
    "    sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
